{#- SPDX-License-Identifier: AGPL-3.0-or-later -#}
{#- Copyright (C) 2025 Mingyuan Xiang -#}

{% if global_mem_buffer %}
#define _LEA_ADD_SIZE {{ lea_buffer_size }}
{%- else %}
#define _LEA_ADD_SIZE {{ lea_src_size }}
{% endif %}
#define _OUTPUT_SIZE {{ out_len }}
#define _LEA_REMAIN_SIZE (_OUTPUT_SIZE % _LEA_ADD_SIZE)
{%- if padding %}
#define _PADDING_TOP {{ padding["top"] }}
#define _PADDING_BOTTOM {{ padding["bottom"] }}
#define _PADDING_RIGHT {{ padding["right"] }}
#define _PADDING_LEFT {{ padding["left"] }}
{%- endif %}
{% if global_mem_buffer %}
/* assign the lea buffer pointer */
#define lea_src (lea_buffer)
#define lea_flt (lea_buffer + {{ lea_src_size }})
#define lea_tmp (lea_buffer + {{ lea_src_size + lea_flt_size }})
#define lea_dst (lea_buffer + {{ lea_src_size + lea_flt_size + lea_tmp_size }})
{% endif %}
#define _INPUT_LINE_SIZE {{ in_line_size }}
{% if stride_row > 1 %}
#define _STRIDE_ROW_SIZE {{ stride_row }}
{%- endif %}
{%- if stride_col > 1 %}
#define _STRIDE_COL_SIZE {{ stride_col }}
{%- endif %}
{%- if flt_col_size % 2 != 0 %}
#define _KERNEL_SIZE_ALIGNED {{ flt_col_size + 1 }}
{%- endif %}
#define _KERNEL_ROW_SIZE {{ flt_size }}
#define _KERNEL_COL_SIZE {{ flt_col_size }}
{% if lea_opt %}
static inline __attribute__((always_inline)) void new_add_q15(const _q15* srcA, const _q15* srcB, _q15* dst) {
  lea_add_params->input2 = MSP_LEA_CONVERT_ADDRESS(srcB);
  lea_add_params->output = MSP_LEA_CONVERT_ADDRESS(dst);

  /* Load source arguments to LEA. */
  LEAPMS0 = MSP_LEA_CONVERT_ADDRESS(srcA);
  LEAPMS1 = MSP_LEA_CONVERT_ADDRESS(lea_add_params);

  /* Invoke the LEACMD__ADDMATRIX command. */
  msp_lea_invokeCommand(LEACMD__ADDMATRIX);
}

static inline __attribute__((always_inline)) void new_fir_q15(_q15* coeffs, const _q15* src, _q15* dst) {
  /* Set MSP_LEA_FIR_PARAMS structure. */
  lea_fir_params->coeffs = MSP_LEA_CONVERT_ADDRESS(coeffs);
  lea_fir_params->output = MSP_LEA_CONVERT_ADDRESS(dst);

  /* Load source arguments to LEA. */
  LEAPMS0 = MSP_LEA_CONVERT_ADDRESS(src);
  LEAPMS1 = MSP_LEA_CONVERT_ADDRESS(lea_fir_params);

  /* Invoke the LEACMD__FIR command. */
  msp_lea_invokeCommand(LEACMD__FIR);
}

static inline __attribute__((always_inline)) void set_offset(int16_t offset) {
  offset_vector[0] = offset;
  offset_vector[1] = offset;
}

static inline __attribute__((always_inline)) void new_offset_q15(const _q15* src, _q15* dst) {
  lea_offset_params->output = MSP_LEA_CONVERT_ADDRESS(dst);

  /* Load source arguments to LEA. */
  LEAPMS0 = MSP_LEA_CONVERT_ADDRESS(src);
  LEAPMS1 = MSP_LEA_CONVERT_ADDRESS(lea_offset_params);

  /* Invoke the LEACMD__ADDMATRIX command. */
  msp_lea_invokeCommand(LEACMD__ADDMATRIX);
}
{% if stride_col > 1 %}
static inline __attribute__((always_inline)) void new_deinterleave_q15(const _q15* src, _q15* dst) {
  lea_deinterleave_params->output = MSP_LEA_CONVERT_ADDRESS(dst);

  /* Load source arguments to LEA. */
  LEAPMS0 = MSP_LEA_CONVERT_ADDRESS(&src[deinterleave_channel]);
  LEAPMS1 = MSP_LEA_CONVERT_ADDRESS(lea_deinterleave_params);

  /* Invoke the command. */
  msp_lea_invokeCommand(deinterleave_cmdId);
}
{%- endif %}
{%- endif %}

void {{ layer_name }}(mat_t* input, mat_t* output, mat_t* weight, mat_t* bias) {
  uint16_t in_channels = input->dims[1];
  uint16_t out_channels = output->dims[1];
  uint16_t output_len = output->strides[1];
  uint16_t input_len = input->strides[1];
  uint16_t kernel_row_size = _KERNEL_ROW_SIZE;
  uint16_t kernel_col_size = _KERNEL_COL_SIZE;

  uintptr_t src_lea_addr = (uintptr_t)lea_src;
  uintptr_t dst_lea_addr = (uintptr_t)lea_dst;

  uintptr_t flt_lea_addr = (uintptr_t)lea_flt;
  {%- if flt_col_size % 2 == 0 %}
  uint16_t flt_len = weight->strides[1];
  uintptr_t flt_addr_offset = flt_len * sizeof(int16_t);
  {%- endif %}
  uintptr_t flt_addr_col_offset = kernel_col_size * sizeof(int16_t);
  uintptr_t flt_addr_padding_offset = (kernel_col_size + 1) * sizeof(int16_t);
  uintptr_t input_lea_addr = src_lea_addr;
  uintptr_t input_fram_addr = (uintptr_t)(input->data);
  uintptr_t input_channel_addr = input_fram_addr;
  uintptr_t input_channel_offset = input_len * sizeof(int16_t);
  uintptr_t output_lea_addr = dst_lea_addr;
  uintptr_t output_addr_offset = output_len * sizeof(int16_t);

  int16_t* conv_flt = lea_flt;
  uint16_t input_line_size = input->dims[3];
  uint16_t output_line_size = output->dims[3];
  uint16_t output_line_num = output->dims[2];
  uint16_t output_line_size_offset = output_line_size * sizeof(int16_t);
  uint16_t input_line_size_offset = input_line_size * sizeof(int16_t);

  uint16_t lea_remain_size = _LEA_REMAIN_SIZE;
  {%- if (global_mem_buffer and (out_len % lea_buffer_size) != 0) or ((not global_mem_buffer) and (out_len % lea_src_size) != 0) %}
  uintptr_t output_remain_size_offset = lea_remain_size * sizeof(int16_t);
  {%- endif %}
  {%- if (global_mem_buffer and out_len >= lea_buffer_size) or ((not global_mem_buffer) and out_len >= lea_src_size) %}
  uintptr_t output_lea_min_size_offset = _LEA_ADD_SIZE * sizeof(int16_t);
  {%- endif %}
  uint16_t lea_remain_size_aligned = MAKE_ALIGN_2(lea_remain_size);
  {% if stride_row > 1 %}
  uint16_t input_line_strided_size_offset = _INPUT_LINE_SIZE * _STRIDE_ROW_SIZE * sizeof(int16_t);
  {%- endif %}

  {% if lea_opt %}
  {%- if stride_col > 1 %}
  deinterleave_init(MAKE_ALIGN_2(output_line_size), 0, _STRIDE_COL_SIZE);
  {%- endif %}
  uint16_t tapLength = MAKE_ALIGN_2(kernel_col_size);
  add_init(MAKE_ALIGN_2(output_line_size));
  fir_init(MAKE_ALIGN_2(input_line_size - kernel_col_size + 1), tapLength);
  offset_init(lea_remain_size_aligned);
  {%- else %}
  {%- if stride_col > 1 %}
  msp_deinterleave_q15_params deinterleave_params = {
    .length = MAKE_ALIGN_2(output_line_size),
    .channel = 0,
    .numChannels = _STRIDE_COL_SIZE
  };
  {%- endif %}
  msp_fir_q15_params conv_params = {
    .length = MAKE_ALIGN_2(input_line_size - kernel_col_size + 1),
    .tapLength = MAKE_ALIGN_2(kernel_col_size),
    .coeffs = lea_flt,
    .enableCircularBuffer = false 
  };
  msp_add_q15_params add_params = { .length = MAKE_ALIGN_2(output_line_size) };
  msp_offset_q15_params offset_params = { .length = lea_remain_size_aligned, .offset = 0 };
  {%- endif %}
  {%- if has_adaptive_gen_mem %}
  uint16_t s;
  {%- if flt_col_size < adaptive_gen_mem_size %}
  uint16_t flt_lea_pos = 0;
  uint16_t flt_fram_pos = 0;
  {%- endif %}

  {%- if out_line_size < adaptive_gen_mem_size %}
  uint16_t output_fram_pos = 0;
  {%- endif %}

  {%- if in_line_size < adaptive_gen_mem_size %}
  uint16_t input_fram_pos = 0;
  uint16_t input_channel_pos = 0;

  {%- if stride_row > 1 %}
  uint16_t input_line_strided_size = _INPUT_LINE_SIZE * _STRIDE_ROW_SIZE;
  {%- endif %}
  {%- endif %}
  {%- endif %}

  {%- if dma_opt %}
  uint16_t zero = 0;
  {%- endif %}

  {% if flt_col_size % 2 != 0 %}
  /* set the aligned position to be zeros */
  uint16_t lea_reset_pos = 0;
  for (uint16_t k = 0; k < kernel_row_size; ++k) {
    lea_flt[lea_reset_pos] = 0;
    lea_reset_pos += (kernel_col_size + 1);
  }
  {%- endif %}

  lea_src[_INPUT_LINE_SIZE] = 0;
  lea_src[_INPUT_LINE_SIZE + 1] = 0;

  uintptr_t intermittent_buffer_addr = (uintptr_t)intermittent_buffer;

  /* No need for double buffering (for loop indices) as each output is independent */
  if (intermittent_status[COMPUTE_CK] == INTERMITTENT_{{ layer_name }}_PREPARE) {
    {%- if dma_opt or padding %}
    uintptr_t output_fram_addr = (uintptr_t)(output->data);
    {%- endif %}
    switch (intermittent_status[COMPUTE_SWITCH]) {
    case PAD_START: {
    {%- if padding %}
      large_memset(output->data, GET_MAT_SIZE(input), COMPUTE_IO_COL);
      VOLATILE_WRITE(PAD_MAIN, COMPUTE_SWITCH);
    }
    case PAD_MAIN: {
    /* Pad the input for ({{ padding["left"] }}, {{ padding["right"] }}, {{ padding["top"] }}, {{ padding["bottom"] }}) */
      uint16_t input_line_num = input->dims[2] - _PADDING_TOP - _PADDING_BOTTOM;
      uint16_t input_line_size_bf = input->dims[3] - _PADDING_RIGHT - _PADDING_LEFT;
      uint16_t input_len_bf = input_line_num * input_line_size_bf;

      if (intermittent_status[COMPUTE_IO_ROW] >= input_line_num) {
        VOLATILE_WRITE(0, COMPUTE_IO_ROW);
        uint16_t next_i = intermittent_status[COMPUTE_IN_CH] + 1;
        VOLATILE_WRITE(next_i, COMPUTE_IN_CH);
      }

      _q15* padding_ptr_in = input->data + \
        intermittent_status[COMPUTE_IN_CH] * input_len_bf + \
        intermittent_status[COMPUTE_IO_ROW] * input_line_size_bf;
      _q15* padding_ptr_out = output->data + \
        intermittent_status[COMPUTE_IN_CH] * input_len + \
        intermittent_status[COMPUTE_IO_ROW] * input_line_size;

      for (uint16_t i = intermittent_status[COMPUTE_IN_CH]; i < in_channels; ++i) {
        {%- for n in range(padding["top"]) %}
        padding_ptr_out += input_line_size;
        {%- endfor %}
        for (uint16_t j = intermittent_status[COMPUTE_IO_ROW]; j < input_line_num; ++j) {
          padding_ptr_out += _PADDING_LEFT;
          {%- if has_adaptive_gen_mem and (in_line_size - padding["left"] - padding["right"] < adaptive_gen_mem_size) %}
          {%- for n in range(in_line_size - padding["left"] - padding["right"]) %}
          *padding_ptr_out = *padding_ptr_in;
          ++padding_ptr_out;
          ++padding_ptr_in;
          {%- endfor %}

          padding_ptr_out += _PADDING_RIGHT;
          {%- else %}
          DMA_makeTransfer((uintptr_t)padding_ptr_in, (uintptr_t)padding_ptr_out, input_line_size_bf);
          padding_ptr_in += input_line_size_bf;
          padding_ptr_out += (_PADDING_RIGHT + input_line_size_bf);
          {%- endif %}

          intermittent_status[COMPUTE_IO_ROW]++;
        }
        {%- for n in range(padding["bottom"]) %}
        padding_ptr_out += input_line_size;
        {%- endfor %}

        VOLATILE_WRITE(0, COMPUTE_IO_ROW);
        uint16_t next_i = i + 1;
        VOLATILE_WRITE(next_i, COMPUTE_IN_CH);
      }

      VOLATILE_WRITE(PAD_TRANSFER, COMPUTE_SWITCH);
    }
    case PAD_TRANSFER: {
      large_dma(output_fram_addr, input_fram_addr, GET_MAT_SIZE(input), COMPUTE_OUT_CH);

      VOLATILE_WRITE(PAD_MEMSET, COMPUTE_SWITCH);
    }
    case PAD_MEMSET:
      /* intermittent_status[COMPUTE_IO_ROW] should be zero right now */
      large_memset(output->data, GET_MAT_SIZE(output), COMPUTE_IO_ROW);
      VOLATILE_WRITE(PAD_ST_RESET, COMPUTE_SWITCH);
    case PAD_ST_RESET:
      VOLATILE_WRITE(0, COMPUTE_IO_COL);
      VOLATILE_WRITE(0, COMPUTE_IO_ROW);
      VOLATILE_WRITE(0, COMPUTE_IN_CH);
      VOLATILE_WRITE(0, COMPUTE_OUT_CH);
    {%- else %} {# not padding #}
      large_memset(output->data, GET_MAT_SIZE(output), COMPUTE_IO_COL);
      VOLATILE_WRITE(PAD_ST_RESET, COMPUTE_SWITCH);
    }
    case PAD_ST_RESET:
      VOLATILE_WRITE(0, COMPUTE_IO_COL);
    {%- endif %} 
      VOLATILE_WRITE(PAD_END, COMPUTE_SWITCH);
    default:
      break;
    }

    VOLATILE_WRITE(INTERMITTENT_{{ layer_name }}_MAIN, COMPUTE_CK);
  }

  /* convolution */
  if (intermittent_status[COMPUTE_CK] == INTERMITTENT_{{ layer_name }}_MAIN) {
    /* Recover loop variables */
    if (intermittent_status[COMPUTE_IN_CH] & DOUBLE_BUFFER_WRITE) {
      uint16_t idx = intermittent_status[COMPUTE_IN_CH] & DOUBLE_BUFFER_COMPLETE;
      VOLATILE_WRITE(intermittent_buffer[0], COMPUTE_IO_ROW);
      VOLATILE_WRITE(idx, COMPUTE_IN_CH);
    }
    {% if group == 1 %}
    if (intermittent_status[COMPUTE_OUT_CH] & DOUBLE_BUFFER_WRITE) {
      uint16_t idx = intermittent_status[COMPUTE_OUT_CH] & DOUBLE_BUFFER_COMPLETE;
      VOLATILE_WRITE(intermittent_buffer[0], COMPUTE_IN_CH);
      VOLATILE_WRITE(idx, COMPUTE_OUT_CH);
    }
    {%- endif %}

    if (intermittent_status[COMPUTE_IO_ROW] & DOUBLE_BUFFER_WRITE) {
      /* Start transferring to output buffer. Recover from temporary buffer */
      uint16_t line = intermittent_status[COMPUTE_IO_ROW] & DOUBLE_BUFFER_COMPLETE;
      {%- if group == 1 %}
      uint16_t offset = intermittent_status[COMPUTE_OUT_CH] * output_len + \
        (line - 1) * output_line_size;
      {%- else %}
      uint16_t offset = intermittent_status[COMPUTE_IN_CH] * output_len + \
        (line - 1) * output_line_size;
      {%- endif %}

      {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
      {%- for n in range(out_line_size) %}
      output->data[offset] = intermittent_buffer[{{ loop.index0 }}];
      offset++;
      {%- endfor %}
  
      {%- else %}
      uintptr_t addr = (uintptr_t)(output->data) + offset * sizeof(int16_t);
      DMA_makeTransfer(intermittent_buffer_addr, addr, output_line_size);
      {%- endif %}

      VOLATILE_WRITE(line, COMPUTE_IO_ROW);
    }

    if (intermittent_status[COMPUTE_IO_ROW] >= output_line_num) {
      uint16_t next_idx = intermittent_status[COMPUTE_IN_CH] + 1;
      DOUBLE_BUFFER_ASSIGN(next_idx, COMPUTE_IN_CH, 0, intermittent_status[COMPUTE_IO_ROW]);
    }
    {% if group == 1 %}
    if (intermittent_status[COMPUTE_IN_CH] >= in_channels) {
      uint16_t next_idx = intermittent_status[COMPUTE_OUT_CH] + 1;
      DOUBLE_BUFFER_ASSIGN(next_idx, COMPUTE_OUT_CH, 0, intermittent_status[COMPUTE_IN_CH]);
    }
    {%- endif %}

    {%- if group == 1 %}
    {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
    output_fram_pos = intermittent_status[COMPUTE_OUT_CH] * output_len;
    {%- else %}
    uintptr_t output_fram_addr = (uintptr_t)(output->data) + \
      intermittent_status[COMPUTE_OUT_CH] * output_addr_offset;
    {%- endif %}
    {%- if has_adaptive_gen_mem and ((flt_col_size % 2 != 0 and flt_col_size < adaptive_gen_mem_size) or (flt_col_size % 2 == 0 and flt_len < adaptive_gen_mem_size)) %}
    flt_fram_pos = intermittent_status[COMPUTE_OUT_CH] * weight->strides[0] + \
      intermittent_status[COMPUTE_IN_CH] * weight->strides[1];
    {%- else %}
    uintptr_t flt_fram_addr = (uintptr_t)(weight->data) + \
      (intermittent_status[COMPUTE_OUT_CH] * weight->strides[0] + \
        intermittent_status[COMPUTE_IN_CH] * weight->strides[1]) * sizeof(int16_t);
    {%- endif %}

    {%- else %} {# if group != 1 #}
    {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
    output_fram_pos = intermittent_status[COMPUTE_IN_CH] * output_len;
    {%- else %}
    uintptr_t output_fram_addr = (uintptr_t)(output->data) + \
      intermittent_status[COMPUTE_IN_CH] * output_addr_offset;
    {%- endif %}
    {%- if has_adaptive_gen_mem and ((flt_col_size % 2 != 0 and flt_col_size < adaptive_gen_mem_size) or (flt_col_size % 2 == 0 and flt_len < adaptive_gen_mem_size)) %}
    flt_fram_pos = intermittent_status[COMPUTE_IN_CH] * weight->strides[0];
    {%- else %}
    uintptr_t flt_fram_addr = (uintptr_t)(weight->data) + \
      intermittent_status[COMPUTE_IN_CH] * weight->strides[0] * sizeof(int16_t);
    {%- endif %}
    {%- endif %}

  {% if group == 1 %}
    for (uint16_t i = intermittent_status[COMPUTE_OUT_CH]; i < out_channels; ++i) {
  {%- else %}
    {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
    uint16_t tmp_output_pos = output_fram_pos + \
      intermittent_status[COMPUTE_IO_ROW] * output_line_size;
    {%- else %}
    uintptr_t tmp_output_addr = output_fram_addr + \
      intermittent_status[COMPUTE_IO_ROW] * output_line_size_offset;
    {%- endif %}
  {%- endif %}
      {%- if has_adaptive_gen_mem and in_line_size < adaptive_gen_mem_size %}
      input_fram_pos = intermittent_status[COMPUTE_IN_CH] * input_len;
      {%- else %}
      input_fram_addr = (uintptr_t)(input->data) + \
        intermittent_status[COMPUTE_IN_CH] * input_channel_offset;
      {%- endif %}

      for (uint16_t j = intermittent_status[COMPUTE_IN_CH]; j < in_channels; ++j) {
        {%- if group == 1 %}
        {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
        uint16_t tmp_output_pos = output_fram_pos + \
          intermittent_status[COMPUTE_IO_ROW] * output_line_size;
        {%- else %}
        uintptr_t tmp_output_addr = output_fram_addr + \
          intermittent_status[COMPUTE_IO_ROW] * output_line_size_offset;
        {%- endif %}
        {%- endif %}

        {%- if has_adaptive_gen_mem and in_line_size < adaptive_gen_mem_size %}
        {%- if stride_row > 1 %}
        input_channel_pos = input_fram_pos + \
          intermittent_status[COMPUTE_IO_ROW] * input_line_strided_size;
        {%- else %}
        input_channel_pos = input_fram_pos + \
          intermittent_status[COMPUTE_IO_ROW] * input_line_size;
        {%- endif %}
        {%- else %}

        {%- if stride_row > 1 %}
        input_channel_addr = input_fram_addr + \
          intermittent_status[COMPUTE_IO_ROW] * input_line_strided_size_offset;
        {%- else %}
        input_channel_addr = input_fram_addr + \
          intermittent_status[COMPUTE_IO_ROW] * input_line_size_offset;
        {%- endif %}
        {%- endif %}

        /* send kernel to LEA RAM */
        {%- if flt_col_size % 2 != 0 %}
        /*
        * pad zero to the beginning of the filter if the filter's size is
        * not aligned to 2
        */
        {%- if has_adaptive_gen_mem and flt_col_size < adaptive_gen_mem_size %}
        ++flt_lea_pos;
        {%- else %}
        flt_lea_addr += sizeof(uint16_t);
        {%- endif %}

        for (uint16_t k = 0; k < kernel_row_size; ++k) {
          {%- if has_adaptive_gen_mem and flt_col_size < adaptive_gen_mem_size %}
          {%- for n in range(flt_col_size) %}
          lea_flt[flt_lea_pos] = weight->data[flt_fram_pos];
          flt_lea_pos++;
          flt_fram_pos++;
          {%- endfor %}

          flt_lea_pos++;

          {%- else %}
          DMA_makeTransfer(flt_fram_addr, flt_lea_addr, kernel_col_size);
          flt_lea_addr += flt_addr_padding_offset;
          flt_fram_addr += flt_addr_col_offset;
          {%- endif %}
        }
        /* restore flt_lea_addr pointer to the beginning of the array */
        {%- if has_adaptive_gen_mem and flt_col_size < adaptive_gen_mem_size %}
        flt_lea_pos = 0;
        {%- else %}
        flt_lea_addr = (uintptr_t)lea_flt;
        {%- endif %}

        {%- else %}

        {%- if has_adaptive_gen_mem and flt_len < adaptive_gen_mem_size %}
        {%- for n in range(flt_len) %}
        lea_flt[{{ loop.index0 }}] = weight->data[flt_fram_pos];
        flt_fram_pos++;
        {%- endfor %}

        {%- else %}
        DMA_makeTransfer(flt_fram_addr, flt_lea_addr, flt_len);
        flt_fram_addr += flt_addr_offset;
        {%- endif %}

        {%- endif %}

        for (uint16_t l = intermittent_status[COMPUTE_IO_ROW]; l < output_line_num; ++l) {
          {%- if has_adaptive_gen_mem and in_line_size < adaptive_gen_mem_size %}
          uint16_t tmp_input_pos = input_channel_pos;
          {%- else %}
          uintptr_t tmp_input_addr = input_channel_addr;
          {%- endif %}
          /* send output to LEA RAM */
          {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
          s = tmp_output_pos;
          {%- for n in range(out_line_size) %}
          lea_dst[{{ loop.index0 }}] = output->data[s];
          s++;
          {%- endfor %}

          {%- else %}
          DMA_makeTransfer(tmp_output_addr, output_lea_addr, output_line_size);
          {%- endif %}

          conv_flt = lea_flt;

          for (uint16_t k = 0; k < kernel_row_size; ++k) {
            /* send input to LEA RAM */
            {%- if has_adaptive_gen_mem and in_line_size < adaptive_gen_mem_size %}
            {%- for n in range(in_line_size) %}
            lea_src[{{ loop.index0 }}] = input->data[tmp_input_pos];
            tmp_input_pos++;
            {%- endfor %}

            {%- else %}
            DMA_makeTransfer(tmp_input_addr, input_lea_addr, input_line_size);
            {%- endif %}

            {%- if not lea_opt %}
            conv_params.coeffs = conv_flt;
            {%- endif %}

            /* convolution */
            {%- if lea_opt %}
            new_fir_q15(conv_flt, lea_src, lea_tmp);
            {%- else %}
            msp_fir_q15(&conv_params, lea_src, lea_tmp);
            {%- endif %}
            {%- if stride_col > 1 %}
            /*
            * stride
            * Use lea deinterleave and store it in the lea_src
            */
            {% if lea_opt %}
            new_deinterleave_q15(lea_tmp, lea_src);

            /* accumulate results for a 2D convolution */
            new_add_q15(lea_dst, lea_src, lea_dst);
            {%- else %}
            msp_deinterleave_q15(&deinterleave_params, lea_tmp, lea_src);

            /* accumulate results for a 2D convolution */
            msp_add_q15(&add_params, lea_dst, lea_src, lea_dst);
            {%- endif %}

            {%- else %}

            /* accumulate results for a 2D convolution */
            {%- if lea_opt %}
            new_add_q15(lea_dst, lea_tmp, lea_dst);
            {%- else %}
            msp_add_q15(&add_params, lea_dst, lea_tmp, lea_dst);
            {%- endif %}
            {%- endif %}

            {%- if lea_opt %}
            conv_flt += tapLength;
            {%- else %}
            conv_flt += conv_params.tapLength;
            {%- endif %}

            {%- if (not has_adaptive_gen_mem) or (in_line_size >= adaptive_gen_mem_size) %}
            tmp_input_addr += input_line_size_offset;
            {%- endif %}
          }

          /* bring back output from LEA RAM */
          {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
          uint16_t next_l = l + 1;
          {%- for n in range(out_line_size) %}
          intermittent_buffer[{{ loop.index0 }}] = lea_dst[{{ loop.index0 }}];
          {%- endfor %}
          next_l = next_l | DOUBLE_BUFFER_WRITE;
          VOLATILE_WRITE(next_l, COMPUTE_IO_ROW);
          {%- for n in range(out_line_size) %}
          output->data[tmp_output_pos] = lea_dst[{{ loop.index0 }}];
          tmp_output_pos++;
          {%- endfor %}
          next_l = next_l & DOUBLE_BUFFER_COMPLETE;
          VOLATILE_WRITE(next_l, COMPUTE_IO_ROW);

          {%- else %}
          uint16_t next_l = l + 1;
          DOUBLE_BUFFER_TRANSFER(next_l, COMPUTE_IO_ROW, output_lea_addr, tmp_output_addr, output_line_size);
          {%- endif %}
          {% if (not has_adaptive_gen_mem) or (out_line_size >= adaptive_gen_mem_size) %}
          tmp_output_addr += output_line_size_offset;
          {%- endif %}

          {%- if has_adaptive_gen_mem and in_line_size < adaptive_gen_mem_size %}
          {%- if stride_row > 1 %}
          input_channel_pos += input_line_strided_size;
          {%- else %}
          input_channel_pos += input_line_size;
          {%- endif %}

          {%- else %}
          {%- if stride_row > 1 %}
          input_channel_addr += input_line_strided_size_offset;
          {%- else %}
          input_channel_addr += input_line_size_offset; 
          {%- endif %}

          {%- endif %}
        }
        {%- if has_adaptive_gen_mem and in_line_size < adaptive_gen_mem_size %}
        input_fram_pos += input_len;
        {%- else %}
        input_fram_addr += input_channel_offset;
        {%- endif %}

        uint16_t next_j = j + 1;
        DOUBLE_BUFFER_ASSIGN(next_j, COMPUTE_IN_CH, 0, intermittent_status[COMPUTE_IO_ROW]);
      }
  {%- if group == 1 %}
      {%- if has_adaptive_gen_mem and out_line_size < adaptive_gen_mem_size %}
      output_fram_pos += output_len;
      {%- else %}
      output_fram_addr += output_addr_offset;
      {%- endif %}

      uint16_t next_i = i + 1;
      DOUBLE_BUFFER_ASSIGN(next_i, COMPUTE_OUT_CH, 0, intermittent_status[COMPUTE_IN_CH]);
    }
  {%- endif %}

    VOLATILE_WRITE(INTERMITTENT_{{ layer_name }}_BIAS, COMPUTE_CK);
  }

  /* add bias and left shift */
  if (intermittent_status[COMPUTE_CK] == INTERMITTENT_{{ layer_name }}_BIAS) {
    _q15* lea_add = lea_src;
    uint16_t add_size = _LEA_ADD_SIZE;
    uintptr_t lea_add_addr = (uintptr_t)lea_add;
    {% if group == 1 %}
    /* Recover loop variables. We use COMPUTE_IN_CH as it will be zero when the start of this */
    if (intermittent_status[COMPUTE_IN_CH] & DOUBLE_BUFFER_WRITE) {
      uint16_t idx = intermittent_status[COMPUTE_IN_CH] & DOUBLE_BUFFER_COMPLETE;
      VOLATILE_WRITE(intermittent_buffer[0], COMPUTE_IO_ROW);
      VOLATILE_WRITE(idx, COMPUTE_IN_CH);
    }
    {%- else %}
    /* Recover loop variables. We use COMPUTE_OUT_CH as it will be zero when the start of this */
    if (intermittent_status[COMPUTE_OUT_CH] & DOUBLE_BUFFER_WRITE) {
      uint16_t idx = intermittent_status[COMPUTE_OUT_CH] & DOUBLE_BUFFER_COMPLETE;
      VOLATILE_WRITE(intermittent_buffer[0], COMPUTE_IO_ROW);
      VOLATILE_WRITE(idx, COMPUTE_OUT_CH);
    }
    {%- endif %}

    if (intermittent_status[COMPUTE_IO_ROW] & DOUBLE_BUFFER_WRITE) {
      /* Start transferring to output buffer. Recover from temporary buffer */
      uint16_t line = intermittent_status[COMPUTE_IO_ROW] & DOUBLE_BUFFER_COMPLETE;
      uint16_t size = line;
      if (size > lea_remain_size) {
        size = add_size;
      }
      {%- if group == 1 %}
      uintptr_t addr = (uintptr_t)(output->data) + \
        (intermittent_status[COMPUTE_IN_CH] * output_len + (line - size)) * sizeof(int16_t);
      {%- else %}
      uintptr_t addr = (uintptr_t)(output->data) + \
        (intermittent_status[COMPUTE_OUT_CH] * output_len + (line - size)) * sizeof(int16_t);
      {%- endif %}

      DMA_makeTransfer(intermittent_buffer_addr, addr, size);

      VOLATILE_WRITE(line, COMPUTE_IO_ROW);
    }

    {%- if group == 1 %}
    if (intermittent_status[COMPUTE_IO_ROW] >= output_len) {
      uint16_t next_idx = intermittent_status[COMPUTE_IN_CH] + 1;
      DOUBLE_BUFFER_ASSIGN(next_idx, COMPUTE_IN_CH, 0, intermittent_status[COMPUTE_IO_ROW]);
    }

    uintptr_t output_fram_addr = (uintptr_t)(output->data) + \
      (intermittent_status[COMPUTE_IN_CH] * output_len + \
        intermittent_status[COMPUTE_IO_ROW]) * sizeof(int16_t);

    for (uint16_t i = intermittent_status[COMPUTE_IN_CH]; i < out_channels; ++i) {
    {%- else %}
    if (intermittent_status[COMPUTE_IO_ROW] >= output_len) {
      uint16_t next_idx = intermittent_status[COMPUTE_OUT_CH] + 1;
      DOUBLE_BUFFER_ASSIGN(next_idx, COMPUTE_OUT_CH, 0, intermittent_status[COMPUTE_IO_ROW]);
    }

    uintptr_t output_fram_addr = (uintptr_t)(output->data) + \
      (intermittent_status[COMPUTE_OUT_CH] * output_len + \
        intermittent_status[COMPUTE_IO_ROW]) * sizeof(int16_t);

    for (uint16_t i = intermittent_status[COMPUTE_OUT_CH]; i < out_channels; ++i) {
    {%- endif %}
      {%- if lea_opt %}
      set_offset(bias->data[i]);
      {%- else %}
      offset_params.offset = bias->data[i];
      {%- endif %}

      {%- if (global_mem_buffer and (out_len % lea_buffer_size) != 0) or ((not global_mem_buffer) and (out_len % lea_src_size) != 0) %}
      {%- if (global_mem_buffer and out_len >= lea_buffer_size) or ((not global_mem_buffer) and out_len >= lea_src_size) %}
      if (intermittent_status[COMPUTE_IO_ROW] == 0) {
      {%- endif %}
        {%- if lea_opt %}
        lea_offset_params->vectorSize = lea_remain_size_aligned;
        {%- if qf > 0 %}
        lea_add_params->vectorSize = lea_remain_size_aligned;
        {%- endif %}
        {%- else %}
        offset_params.length = lea_remain_size_aligned;
        {%- if qf > 0 %}
        add_params.length = lea_remain_size_aligned;
        {%- endif %}
        {%- endif %}

        DMA_makeTransfer(output_fram_addr, lea_add_addr, lea_remain_size);
        {% for i in range(qf) %}
        {%- if lea_opt %}
        new_add_q15(lea_add, lea_add, lea_add);
        {%- else %}
        msp_add_q15(&add_params, lea_add, lea_add, lea_add);
        {%- endif %}
        {%- endfor %}
        {%- if lea_opt %}
        new_offset_q15(lea_add, lea_add);
        {%- else %}
        msp_offset_q15(&offset_params, lea_add, lea_add);
        {%- endif %}

        uint16_t next_r = lea_remain_size;
        DOUBLE_BUFFER_TRANSFER(next_r, COMPUTE_IO_ROW, lea_add_addr, output_fram_addr, lea_remain_size);

        output_fram_addr += output_remain_size_offset;
      {%- if (global_mem_buffer and out_len >= lea_buffer_size) or ((not global_mem_buffer) and out_len >= lea_src_size) %}
      }
      {%- endif %}
      {%- endif %}

      {%- if (global_mem_buffer and out_len >= lea_buffer_size) or ((not global_mem_buffer) and out_len >= lea_src_size) %}

      {%- if lea_opt %}
      lea_offset_params->vectorSize = add_size;
      {%- if qf > 0 %}
      lea_add_params->vectorSize = add_size;
      {%- endif %}
      {%- else %}
      offset_params.length = add_size;
      {%- if qf > 0 %}
      add_params.length = add_size;
      {%- endif %}
      {%- endif %}

      for (uint16_t j = intermittent_status[COMPUTE_IO_ROW]; j < output_len; j += add_size) {
        DMA_makeTransfer(output_fram_addr, lea_add_addr, add_size);
        {% for i in range(qf) %}
        {%- if lea_opt %}
        new_add_q15(lea_add, lea_add, lea_add);
        {%- else %}
        msp_add_q15(&add_params, lea_add, lea_add, lea_add);
        {%- endif %}
        {%- endfor %}
        {%- if lea_opt %}
        new_offset_q15(lea_add, lea_add);
        {%- else %}
        msp_offset_q15(&offset_params, lea_add, lea_add);
        {%- endif %}

        uint16_t next_r = j + add_size;
        DOUBLE_BUFFER_TRANSFER(next_r, COMPUTE_IO_ROW, lea_add_addr, output_fram_addr, add_size);

        output_fram_addr += output_lea_min_size_offset;
      }
      {%- endif %}
      {% if group == 1 %}
      uint16_t next_i = i + 1;
      DOUBLE_BUFFER_ASSIGN(next_i, COMPUTE_IN_CH, 0, intermittent_status[COMPUTE_IO_ROW]);
      {%- else %}
      uint16_t next_i = i + 1;
      DOUBLE_BUFFER_ASSIGN(next_i, COMPUTE_OUT_CH, 0, intermittent_status[COMPUTE_IO_ROW]);
      {%- endif %}
    }

    VOLATILE_WRITE(INTERMITTENT_{{ layer_name }}_EXIT, COMPUTE_CK);
  }

  if (intermittent_status[COMPUTE_CK] == INTERMITTENT_{{ layer_name }}_EXIT) {
    {%- if lea_opt %}
    offset_clear();
    fir_clear();
    add_clear();
    {%- if stride_col > 1 %}
    deinterleave_clear();
    {%- endif %}
    {%- endif %}
    intermittent_status[COMPUTE_SWITCH] = PAD_START;
    intermittent_status[COMPUTE_IO_ROW] = 0;
    intermittent_status[COMPUTE_IN_CH] = 0;
    intermittent_status[COMPUTE_OUT_CH] = 0;
  }
}
